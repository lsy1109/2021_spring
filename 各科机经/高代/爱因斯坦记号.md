# 爱因斯坦记号

- $vector$用底标表示：比如$e_1$，$v_1$
- $vector$的坐标用上标表示：比如$v=\begin{bmatrix}v^x \\v^y \\v^z\end{bmatrix}$，这个式子是因为实际上因为$v=\begin{bmatrix}e_x&e_y  &e_z\end{bmatrix}\begin{bmatrix}v^x\\v^y \\v^z\end{bmatrix}$，并且我们经常用$(v^i)$来表示一个向量。
- $dual$ $vector$用上标表示：比如$\alpha^1$，$\alpha^2$。
- $dual$ $vecotor$的坐标用下标表示，比如$\alpha=\begin{bmatrix}a_1&a_2  &a_3\end{bmatrix}$，理由同上。



- $\alpha(v)=\sum a_iv^i$，这个和和$i$无关，和$basis$无关。
- 任何情况下，如果出现了上标和下标都有$i$，那么就把它$sum$ $up$。但如果上下标一个是$i$一个是$j$，那么就不做其他操作。



- 一个$(a,b)tensor$的坐标：回忆$(a,b)tensor$的意义，$T_{b}^{a}(V)=V^{\otimes a}\otimes (V^*)^{\otimes b}$，$a$个$v\in V$ $tensor$上 $b$个$ v^*\in V^*$。故而$(a,b)tensor$写为：$T=\sum T_{j_{1}, \ldots, j_{b}}^{i_{1}, \ldots, i_{a}} \boldsymbol{e}_{i_{1}} \otimes \cdots \otimes \boldsymbol{e}_{i_{a}} \otimes \boldsymbol{e}_{j_{1}}^{\mathrm{T}} \otimes \cdots \otimes \boldsymbol{e}_{j_{b}}^{\mathrm{T}}$，注意理解这个写法的正确性。

  还是按照之前的叙述，$vector$的坐标为上标，$vector$本身为下标，$dual$ $vector$的坐标为下标，本身为上标。这里用了转置来表示$dual$ $vector$，但还是把坐标写在$T$的下标。



- $matrix$  $A: V\to V$，是一个$T_{1}^{1}(V)tenseor$，故而他的各个坐标是$A^i_j$，也就是$i$行$j$列，对应$A^i_j e_i\otimes \alpha^j$，如果我们$input$一个向量$(v^j)$，那么就是在计算$(A^i_j)(v^j)$，也就是$add$ $up$所有的$j$，得到的$output$是$A^i$，还是一个向量。
- 另外，和vector一样，我们也用($A^i_j$)来简单地代表一个矩阵。



- 什么是$A^i_i$，注意到前文说过，每当同时出现$i$时，就把所有可能的$i$加起来，故而我们把所有的$i-i-entry$加起来，得到的就是$trace$。这里体现出了$trace$和$i$无关，也就是和坐标无关，也就是和$basis$无关。(相似矩阵的$trace$守恒)



- 考虑矩阵的乘法：$A=(A^i_j)$与$B=(B^i_j)$，$(AB)^i_j$的展开式就是对于$(A^i_k)(B^k_j)$来$add$ $up$所有的$k$，故而$(AB)^i_j=(A^i_kB^k_j)$。

- 另一个方面，如果我们写反了$A^i_kB^k_j$，写成了$B^k_jA^i_k$，这有什么影响吗？并不，因为我们还是在$add$ $up$所有的$k$。从而可以见得爱因斯坦记号是不在意顺序的。

- 思考下，如果计算$BA$又会如何？这个显而易见，$B^i_kA^k_j$。

- 当$A$和$B$互为逆元时，我们希望$B^i_kA^k_j=A^i_kB^k_j=\delta^i_j$，满足$i=j$时为1，$i\ne j$时为0。

- 对于$ABC$，我们会写成$C_{j}^{k} B_{k}^{l} A_{l}^{i}$。

  

- 这个例子很有趣，考虑一个$vector$ $v=(v^i)$和$dual$ $vector$ $w^T=(w_i)$，那么$v^iw_i$代表着把所有的$i$ $add$ $up$，就是$w^Tv$，是一个内积。但是$v^iw_j$代表着$vw^T$的$i-j-entry$，也就是一个$T_{1}^{1}(V)tenseor$，也就是矩阵$vw^T$。



- 对于两个向量$v=(v^i)$和$w=(w^i)$，$(v^iw^j)$是$v\otimes w$的坐标，故而这是个$T_{0}^{2}(V)tenseor$，但是$(v^iw^i)$是啥？你如何把$i$ $add$ $up?$故而这是个错误表达，同一个$i$不会在上标出现两次，下标同理。



- 怎么表达$bilinear$ $map？$注意到这是个$T_{2}^{0}(V)tenseor$，他会$eat$两个$tensor$。故而$A=(A_{ij})$（注意到这里也是用系数表达了一整个线性映射，写完整还要带上基）。然后$A(v,w)=A_{ij}v^iw^j$，这就是个数。同时，我们注意到$T_{2}^{0}(V)tenseor$其实是个内积，故而我们希望它=$(\delta_{ij})$，满足$i=j$时为1，$i\ne j$时为0。故而我们表达内积，不要写成$\sum_i(v^iw^i)$，这个和上方那一点是矛盾的。我们会写成$v^iw^j\delta_{ij}$。

  ------



# $Inner$ $products$ $of$ $tensors$

内积由一个$T_{2}^{0}(V)tenseor$ $induced$，$g=(g_{ij})$，那么 $<v,w> = g(v, w) = v^iw^jg_{ij}$，注意到每次出现相同的上下标就把它们$add$ $up$，当然需要$g$正定且对称，$g_{ij}=g_{ji}$，并且$<v,w> = g(v, v) = v^iv^jg_{ij}\ge 0$，等号$iff$ $\forall v^i=0$。



## $Lowering$ $the$ $index$

如何把一个$vector$转为对应的$dual$ $vector$？在由$g$诱导的内积结构下，显然$bra$ $map$就是我们想要的。

$bra$ $map$$<|: v \mapsto  <v|$，$would$ $send$ $the$ $vector$ $(v^i)$ $to$ $the$ $dual$ $vector$ $(v^ig_{ij})$

$proof:$输入一个$w=(w^j)$， $<v|(w) = v^ig_{ij}w^j = (v^ig_{ij} )(w^j)$，证明完毕。



## $Uppering$ $the$ $index$

$Riesz$是$bra$的逆映射，我们需要$g$的逆矩阵。定义在$V$空间上的$T_{0}^{2}(V)tenseor$ $(g^{ij})$作为$T_{2}^{0}(V)tenseor$ $(g_{ij})$的逆。它符合以下性质：

- 对称性：$g^{ij}=g^{ji}$。


- 互逆性：$g^{ij}g_{jk}=\delta ^i_k$，注意到$g^{ij}$与$g_{jk}$分别代表两个矩阵的第$i$行$j$列元和第$j$行$k$列元，二者相乘等于$1$ $iff$  $i=k$。
- $g^{ij}g_{ij}=\delta ^i_i=tarce(I)=dim(V)$



$Riesz$ $map$$:<v| \mapsto v$  ，将一个$dual$ $vector$ $(a_i)$送到$(a_ig^{ij})$

$proof:$我们只需要证明$  bra\circ Riesz=I$，故而输入一个$dual (a_i)$，$bra\circ Riesz(a_i)=a_ig^{ij}g_{ij}=a_i$



$Transpose:$转置实际上就是交换上下角标，把$A^i_j$换为$A_i^j$，$e^i$换为$e_i$。



在由$g$诱导的$V$上和$V^*$上的内积空间中，$bra$ $map$与$Riesz$ $map$是同构双射，即他们保内积。

$<<v|,<w|> = (v^ig_{ij})(w^kg_{kl})g^{jl} = v^ig_{ij}w^k(g_{kl}g^{jl}) = v^iw^kg_{ij}(\delta ^j_k
)=v^iw^kg_{ik} = <v,w>$

我们定义了$dual$上的内积之后，就可以定义$tensor$上的内积了。



$\begin{array}{l}
\text { Definition 6.5.8. We define inner products on }(k, t) \text { -tensor space over } V \text { as }\left\langle\left(T_{j_{1}, \ldots, j_{t}}^{i_{1}, \ldots, i_{k}}\right),\left(S_{n_{1}, \ldots, n_{t}}^{m_{1}, \ldots, m_{k}}\right)\right\rangle=\\
T_{j_{1}, \ldots, j_{k}}^{i_{1}, \ldots, i_{k}} S_{n_{1}, \ldots, n_{k}}^{m_{1}, \ldots, m_{k}} g_{i_{1} m_{1}} \ldots g_{i_{k} m_{k}} g^{j_{1} n_{1}} \ldots g^{j_{t} n_{t}} . \text { Here } g_{i j} \text { is the inner product on } V。
\end{array}$

这个定义我也不会用，大意就是对于所有的$dual$和所有的$vector$分别做内积，然后相乘即可，即：

$<α ⊗ v, β ⊗ w>= <α, β><v, w>$

------



# $Alternating$ $tensor$ $and$ $alternization$

**Definition 6.6.1**. A (0, k)-tensor or a (k, 0)-tensor is alternating if swapping a pair of inputs would negate the output.

这个定义很简单，但是我们可以这么理解，保持$tensor$不变，$swap$ $the$ $input$与保持$input$不变，$swap$ $the$ $tensor$是相同的，也即：

$(v_1\otimes v_2)(\alpha^2,\alpha^1)=(v_2\otimes v_1)(\alpha^1,\alpha^2)$



## Example one

- 显然$det$是$alternating$的。

- 考虑二维平面上的一个$vector$ $pair$ $(v,w)$，我们发现他俩围成了一个平行四边形($parallelogram$)。
- *We say the parallelogram is positively oriented if doing v first and then w is going counter-clockwise around this parallelogram. Then the parallelogram will have an oriented area! Note that (v, w) and (w, v) would represent the SAME parallelogram with OPPOSITE orientation. Therefore the measurement of oriented area is alternating! (And it is also easily seen as bilinear.)*
- 这里提前对$oriented$ $area$做一个阐释，这个概念只关心两件事，$orient$与$area$，前者指方向，后者指面积(或者高维面积)，而不是某个具体的形状。

- *Of course, this area is just the determinant $det(v, w)$. By the determinant formula, oriented area is $e^{12} − e^{21}$, where $e^{ij} $是dual vector，而不是dual vector的坐标。 即：$e^T_i ⊗ e^T_j $ as usual.*
- *In $R^3$, pick three vectors and we have oriented volume, also given by the determinant. And the oriented volume tensor is $e^{123} + e^{231} + e^{312} − e^{132} − e^{213} − e^{321}$. So on so forth to higher dimensions and oriented higher dimensional volumes.*





