# 张量空间

~~这一章的logic依然很混乱，但是我居然读懂了，qwq~~

------

## 6.1 多线性映射就是张量运算

$$
\color{blue}{\large{{\text{Given vector spaces }V_1, . . . , V_n \text{ and }W, \\\text{then the “n-input” map} M : V_1 × · · · × V_n→W\\\text{ is multilinear if it is linear in each input.}}}}
$$
$\Large\mathbb{{比如点积，实数的乘法，差积，求det这些都是张量运算。}}$

------

##  6.2 Kronecker tensor product

$\large\mathbb{{multilinear的映射一般不linear，如何用linear研究multilinear？}}$

------

### 6.2.1 对基的作用

映射首先研究他对基的作用。
$$
\color{blue}{\large{\text{Any bilinear map } B : R^m × R^n → V\\\text{ is uniquely determined by its images} B(e_i, e_j )\text{ in
U for all i, j.}}}
$$


### 6.2.2 张量运算分解

对张量运算$R^m×R^n\to V$的分解。
$$
\color{blue}{\large{\text{Any bilinear map }B : R^m × R^n → V\text{ can be decomposed into the Kronecker}\\
K : R^m × R^n → R
^{mn}\text{ and then a linear map }L : R^{
mn} → V .}}
$$
这个很直白，$B=L\circ  K$ 即可，注意到$K$映射是双射，所以研究$B$只用研究$L$，也就是研究$R^m⊗R^n\to V$，因为$R^m⊗R^n=R^{mn}$。



## 6.3 抽象张量运算

### 6.3.1 映射构成线性空间
$$
\color{blue}{\large\text{Note that linear maps from V → W form a vector space.}\\\text { We write this as L(V ; W).
Now, multilinear maps from }V_1, . . . , V_k \to W \text{ also }\\\text{ form a vector space, which we shall call }M(V_1, . . . , V_k; W).}
$$

$\large\text{比如alternating  multilinear maps space M}(R^n, . . . , R^n; R)$$\large\text{其中alternating}意思为交换两个input的R^n，会让输出结果取负号。$

$\large{\text{不加证明：这些映射确实构成了空间，且这个空间的维度为1，}\\\text{基是det映射。}}$

------



### 6.3.2 映射空间的维度

$$
\color{blue}\large{{根据done right上的某个定理，显然从R^n\to R^m的映射是mn维，}}\\\color{blue}\large{{由此可见，}}M(V_1, . . . , V_k; W)空间的维度，就是简单的维度相乘。
$$

------



### 6.3.3 同构映射带来的等价性

对于一个矩阵$A_{m×n}$，我们可以想见，他意味着如下四件事。

- 左乘一个行向量：也即Linear map： $(R^m)^*→(R^n)^*$。
- 右乘一个列向量：也即Linear map： $R^n→R^m$。
- 左乘一个行向量且右乘一个列向量：也即Bilinear map： $R^n×(R^m)^*→R$。

- 一个$m×n$矩阵。

所以这四件事其实本质是相同的。也就是说，如下的三个向量空间是canonical isomorphism，通过canonical bijection 相互转换。
$$
\color{blue}{\large{\text{1. Linear maps } V \rightarrow W . I.e.,  \mathcal{L}(V ; W) .
\\\text{2. Linear maps } W^{*} \rightarrow V^{*} . I.e.,  \mathcal{L}\left(W^{*} ; V^{*}\right) .
\\\text{3. Bilinear maps } V \times W^{*} \rightarrow \mathbb{R} .  I.e.,  \mathcal{M}\left(V, W^{*} ; \mathbb{R}\right) .}}
$$

------



## 6.3.4 张量空间的核心定义

$$
\color{blue}{\text{ For any two finite dimensional vector spaces  V, W , }\\\text{ we define their tensor space }V \otimes W\text{  as the space  }\mathcal{L}\left(V^{*} ; W\right)\text{  of linear maps from  }V^{*}\\\text{  to  W , or the space  }\mathcal{L}\left(W^{*} ; V\right)\text{  of linear maps from  }W^{*}\text{  to  V , or the space}  \mathcal{B}\left(V^{*}, W^{*} ; \mathbb{R}\right)\\\text{  of bilinear maps from  }V^{*} \times W^{*}  \text{  to  }  \mathbb{R}.\\}
$$

上面这段话还会说人话，下面这一段就不会了。我直接把他翻译为人话。

$$
\color{green}{\text{We have a standard bilinear map  }\otimes: V \times W \rightarrow V \otimes W\text{ , such that  }\otimes(\boldsymbol{v}, \boldsymbol{w})=\boldsymbol{v} \otimes \boldsymbol{w}\\\text{  should represent the
linear map that sends each  }\beta \in W^{*}\text{  to  }\beta(\boldsymbol{w}) \boldsymbol{v}\text{ , or the linear map}\\\text{ that sends each  }\alpha \in V^{*}  \text{ to }  \alpha(\boldsymbol{v}) \boldsymbol{w} ,\\\text{ or the bilinear map}\text{ that sends  }(\alpha, \beta) \in V^{*} \times W^{*}\text{  to the number}  \alpha(\boldsymbol{v}) \beta(\boldsymbol{w}) .}
$$

这段定义提到了两个映射，第一个映射是$\text{ bilinear map  }{\otimes: V \times W \rightarrow V \otimes W}$，注意到这个映射本身不属于张量空间，这个map只是用于将我的两个input转为张量空间里的一个元素。$v \otimes w$是$V\otimes W$ 这个张量空间里的元素，它是上文蓝色定义里提到的三种映射，而$v,w$ 是$V,W$ 这两个向量空间里的元素，双射 $\otimes$ 仅仅是转换作用。

接下来马上给出了$v \otimes w$这个映射的性质，这个映射属于$V \otimes W$张量空间，对于这个空间里的每个元素，我们可以视作一个feed过程。给$v \otimes w$ 先feed一个任意的$\alpha \in V^{*}$ ，这个$\alpha$ 就可以喂饱$v \otimes w$里的$v$部分，得到$\alpha(v)\times w$, 这里的$\times$ 就是对$w$ 做数乘。同理，给$v \otimes w$ 先feed一个任意的$\beta \in W^{*}$ ，这个$\beta$ 就可以喂饱$v \otimes w$里的 $w$ 部分，得到$v\times \beta(w)$, 这里的$\times$ 就是对$v$ 做数乘。如果我同时feed $(\alpha, \beta) \in V^{*} \times W^{*}$ ，（这个$\times$ 是指$(\alpha, \beta) 分别\in V^{*} 与 W^{*}$），那么我就可以把$v \otimes w$ 完全喂饱，他就会输出一个$R$，这个$R$就是$\alpha(v)\times \beta(w)$ ，两个实数的乘法。

总而言之，$V \otimes W$ 这个张量空间里的里的元素$v \otimes w$ 就是等待着你feed他对应的dual vector，每feed一个dual vector，得到一个数乘，feed够了就得到一个$R$。

按照这个定义，我们同样可以定义更多个向量空间一同构成的张量空间，每次feed一个对应的dual vector，就少一个向量，多一次数乘。

------



### 6.3.5 例子

- 对于$R^n\otimes (R^m)^{*}$，意味着你要先feed一个$n$维行向量（$R^n$的dual），再feed一个$m$维列向量（$(R^m)^{*}$的dual），然后得到一个$R$。这是不就是一个矩阵吗？所以说，站在这个观点来看，$v\otimes w^T$貌似就是一个rank 1 矩阵$v\cdot w^T$。
- 另一个例子，对于$R\otimes R$，$v\otimes w$ 貌似就是$v\cdot w$ ，我们可以把$v\cdot w$ 理解为一个实数，然而实际上他是符合6.3.4里定义的一个multilinear map，也就是一个张量！

从这个观点出发，$\otimes$符号看上去很离谱，不过在各自的代数系统里，貌似就是最简单的乘法。另一方面，我们注意到了$v\cdot w^T$矩阵只有rank 1，但是显然映射可比rank 1复杂多了。故而$V \otimes W$ 这个张量空间的基就是这些所有的rank 1的矩阵，所有的张量映射都是由这些基映射linear combination而成的。也就是说，所有的矩阵都是由rank 1矩阵linear combination组成的。

------



### 6.3.6 范式双射

前面已经研究了$V \otimes W$里的元素$v \otimes w$就是一个bilinear map，其实这个就是的实际含义是“canonical isomorphism”，也就是说：在$L:V\otimes W \to U$ 与 $B:V\times W \to U$ 这两个映射空间之间是有canonical bijection的。这个bijection是$C:L\to L\circ \otimes$ 其中$\otimes:V\times W\to V\otimes W$，这个证明可以先证明injection，如果$L\circ \otimes$是0映射，那么任意的$L(v ⊗ w) = L ◦ ⊗(v, w) = 0.$从而$L$也是0映射，再说明$C$映射的domain和codomain维度显然相等，那么$C$是双射。

$\text{In general, we have M}(V_1, . . . , V_k; W)\text{ canonically isomorphic to }\\L(V_1 ⊗ · · · ⊗ V_k; W).$



**remark**

spaces U ⊗ V ⊗ W and (U ⊗ V ) ⊗ W and U ⊗ (V ⊗ W) are canoncially the same.
V ⊗ W and W ⊗ V are also canonically the same, 但是彼此之间的转换并非通过单位矩阵实现的，比如前文提到的v⊗w其实是$v\cdot w^T$，那么w⊗v其实是$w\cdot v^T$他们看上去取了转置。也就是说，站在空间的角度，我们可以认为$V ⊗ W$ and $W ⊗ V$ 是同一个空间，毕竟他们是canonical isomorphism，但是他们的元素$v ⊗ w \ne w\otimes v$。



综上所述，我们已经看出来了domain和codomain可以通过加上dual跳来跳去。

<img src="https://vkceyugu.cdn.bspapp.com/VKCEYUGU-3f5ac434-77f3-4bf1-a1c6-ad1deeb5100a/8b37e05a-c2a7-4682-ad44-3efc3f8b61a4.png" style="zoom:50%;" />

综上所述， how to study a multilinear map $M(V_1, . . . , V_k; W)$? Well, we simply study the vector $space V_1∗ ⊗· · · ⊗ V_k∗ ⊗ W.$

------

### 6.3.7 例子

杨sir上课讲过一个boxtrix的例子，也就是咱们需要输入三个向量，分别属于$R^2,R^3,R^4$然后输出一个$R$。

所以实际上我们需要研究的是$(R^2)^{*}⊗(R^3)^{*}⊗(R^4)^{*}$，也就是我们需要研究所有的$(e_{i2}^T)⊗(e_{j3}^T)⊗(e_{k4}^T)$， 比如我们构造一个映射$f=2 \boldsymbol{e}_{1}^{\mathrm{T}} \otimes \boldsymbol{e}_{1}^{\mathrm{T}} \otimes \boldsymbol{e}_{1}^{\mathrm{T}}+4 \boldsymbol{e}_{2}^{\mathrm{T}} \otimes \boldsymbol{e}_{3}^{\mathrm{T}} \otimes \boldsymbol{e}_{3}^{\mathrm{T}}$，那么给$f$ 输入三个向量$u,v,w$，比如$\left[\begin{array}{l}
1 \\
2
\end{array}\right],\left[\begin{array}{l}
1 \\
2 \\
3 \\
4
\end{array}\right],\left[\begin{array}{l}
1 \\
2 \\
3
\end{array}\right]$，就是非常简单的让$\boldsymbol{e}_{1}^{\mathrm{T}} \otimes \boldsymbol{e}_{1}^{\mathrm{T}} \otimes \boldsymbol{e}_{1}^{\mathrm{T}}$作用在$u,v,w$上，然后$\boldsymbol{e}_{2}^{\mathrm{T}} \otimes \boldsymbol{e}_{3}^{\mathrm{T}} \otimes \boldsymbol{e}_{3}^{\mathrm{T}}$作用在$u,v,w$上，再按照系数组合。这个作用就是按照之前6.3.4的法则计算。而这三个dual vector就相当于取坐标的映射而已，非常好算。

------



## 6.4 高阶张量

### 6.4.1 定义

高阶张量听上去很玄乎，实际上按照6.3.4的法则去理解就好了。

$\text { An }(a, b) \text { -tensor over a vector space } V \text { is an element of }\\ V^{\otimes a} \otimes\left(V^{*}\right)^{\otimes b} \text { . }$

也就是$V$这个向量空间，取出$V\otimes V$一共$\otimes a$次，然后再$\otimes V^*$b次。也被写作$T_b^{a}(V)$.

So the calculation works like this:

$\boldsymbol{v}_{1} \otimes \cdots \otimes \boldsymbol{v}_{a} \otimes \alpha_{1} \otimes \cdots \otimes \boldsymbol{\alpha}_{b}\left(\boldsymbol{w}_{1} \otimes \cdots \otimes \boldsymbol{w}_{b} \otimes \beta_{1} \otimes \cdots \otimes \beta_{a}\right)\\=\beta_{1}\left(\boldsymbol{v}_{1}\right) \ldots \beta_{a}\left(\boldsymbol{v}_{a}\right) \alpha_{1}\left(\boldsymbol{w}_{1}\right) \ldots \alpha_{b}\left(\boldsymbol{w}_{b}\right) $

Note that this formula is for rank 1 tensors. A generic  $(a, b)$  -tensor or  $(b, a) $$ -tensor might not have rank one, but they are always a linear combination of rank 1 tensors.

------

### 6.4.2 例子

say you have a  $(a, 0)$  -tensor. Then this corresponds to a multilinear map to  $\mathbb{R}$  who needs to eat  $a$  dual vectors. So I can feed  k  dual vectors (or a  $(0, k)$  -tensor ，因为$(0, k)$  -tensor 需要$k$ 个vector才能喂饱，所以$(0, k)$  -tensor 就是 $k$ 个dual vector)  to it, and get a  $(a-k, 0)$  -tensor.

In general, for an $ (a, b)$  -tensor, you can feed it a $ (b, a)$  -tensor to get a number. I.e., you can think of $ \mathcal{T}_{b}^{a}(V)$  and  $\mathcal{T}_{a}^{b}(V)$  as dual spaces of each other. 

 Anyway, we have the following.  $\left(\mathcal{T}_{b}^{a}(V)\right)^{*}=\mathcal{T}_{a}^{b}(V) .$

------

